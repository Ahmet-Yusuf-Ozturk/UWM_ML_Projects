{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation Recipe\n",
    "\n",
    "## Import Required Libraries\n",
    "\n",
    "We start by importing the essential libraries for data manipulation and model selection. Pandas will handle our data operations, NumPy provides numerical computing capabilities, and KFold from scikit-learn enables cross-validation functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model Validation Class\n",
    "\n",
    "The `__init__` method sets up the initial state of our model validation class. All attributes are initialized to None or empty containers, creating a clean slate for storing our data, models, and validation results throughout the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.X = None\n",
    "    self.y = None\n",
    "    self.blend = None\n",
    "    self.splits = None\n",
    "    self.model_feature_indices = None\n",
    "    self.model_methods = []\n",
    "    self.metafeatures_df = None\n",
    "    self.X_with_metafeatures = None\n",
    "    self.raw_models = None\n",
    "    self.fit_models = dict()\n",
    "    self.fit_blended_models = dict()\n",
    "    self.raw_stacker = None\n",
    "    self.stacker = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model Prediction Methods\n",
    "\n",
    "This method ensures that each model in our ensemble has the appropriate prediction capabilities. It checks whether each model has either a `predict_proba` or `predict` method, then stores the appropriate method name for later use during model fitting and prediction phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_models(self, models):\n",
    "    for model in models:\n",
    "        # model name\n",
    "        model_name = model.__str__().split('(')[0]\n",
    "        \n",
    "        # assert that each model has a predict_proba or predict method\n",
    "        assert any(['predict' in dir(model), 'predict_proba' in dir(model)]), \\\n",
    "            f'\"{model_name} does not have a predict or predict_proba method\"'\n",
    "        \n",
    "        # determine which method to use from each of the models in the list and keep stored\n",
    "        if 'predict_proba' not in dir(model) and 'predict' in dir(model):\n",
    "            print(f'Note: There is no predict_proba method in {model_name}, therefore predict method will be used')\n",
    "            self.model_methods.append('predict')\n",
    "        \n",
    "        if 'predict_proba' in dir(model):\n",
    "            self.model_methods.append('predict_proba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Universal Model Predictor\n",
    "\n",
    "This static method provides a unified interface for making predictions with any model, regardless of whether it uses `predict` or `predict_proba` methods. It automatically selects the appropriate prediction method and handles probability predictions by extracting the positive class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def model_predictor(model, X, method):\n",
    "    \"\"\"\n",
    "    :param model:\n",
    "    :param X:\n",
    "    :param method: 'predict' or 'predict_proba' as per generated list through model validation assertion\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = None\n",
    "    if method == 'predict_proba':\n",
    "        predictions = model.predict_proba(X)[:, 1]\n",
    "    if method == 'predict':\n",
    "        predictions = model.predict(X)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Ensemble Model\n",
    "\n",
    "The main fitting method orchestrates the entire ensemble learning process. It handles data preparation, model validation, feature selection, and implements both simple ensemble and blended ensemble approaches with cross-validation for robust metafeature generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, models, stacker, blend=False, splits=5, model_feature_indices=None):\n",
    "    \"\"\"\n",
    "    :param X:\n",
    "    :param y:\n",
    "    :param models:\n",
    "    :param stacker:\n",
    "    :param blend:\n",
    "    :param splits:\n",
    "    :param model_feature_indices:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # re-initialise so that you can\n",
    "    self.__init__()\n",
    "    \n",
    "    # some saved variables during fit\n",
    "    self.raw_models = models\n",
    "    self.raw_stacker = stacker\n",
    "    self.blend = blend\n",
    "    self.splits = splits\n",
    "    self.model_feature_indices = model_feature_indices\n",
    "    \n",
    "    self.validate_models(models)\n",
    "    \n",
    "    # convert X into a dataframe if not already\n",
    "    if str(type(X)) != \"\"\"<class 'pandas.core.frame.DataFrame'>\"\"\":\n",
    "        X = pd.DataFrame(X).reset_index(drop=True)\n",
    "    # convert y into a dataframe if not already\n",
    "    if str(type(y)) != \"\"\"<class 'pandas.core.frame.DataFrame'>\"\"\":\n",
    "        y = pd.DataFrame(y).reset_index(drop=True)\n",
    "    \n",
    "    if self.model_feature_indices is None:\n",
    "        self.model_feature_indices = [[i for i in range(X.shape[1])] for _ in models]\n",
    "    \n",
    "    self.model_names_list = [model.__str__().split('(')[0] for model in self.raw_models]\n",
    "    self.metafeatures_df = pd.DataFrame(index=[i for i in range(X.shape[0])], columns=self.model_names_list)\n",
    "    \n",
    "    for model, features, method in zip(self.raw_models, self.model_feature_indices, self.model_methods):\n",
    "        # model name\n",
    "        model_name = model.__str__().split('(')[0]\n",
    "        \n",
    "        # train model\n",
    "        X_model_features = X.iloc[:, features]\n",
    "        \n",
    "        metafeatures = None\n",
    "        # blending if required\n",
    "        if self.blend is False:\n",
    "            \n",
    "            model.fit(X_model_features, np.ravel(y))\n",
    "            \n",
    "            metafeatures = self.model_predictor(model, X_model_features, method)\n",
    "            \n",
    "            # store fit model for future metafeature predictions\n",
    "            self.fit_models[model_name] = model\n",
    "        \n",
    "        if self.blend is True:\n",
    "            self.fit_blended_models[model_name] = []\n",
    "            # folder for blending\n",
    "            kf = KFold(n_splits=self.splits)\n",
    "            \n",
    "            # metafeatures\n",
    "            metafeatures = pd.Series(np.zeros(X.shape[0]))\n",
    "            \n",
    "            for idx, (train_idx, meta_idx) in enumerate(kf.split(X_model_features)):\n",
    "                # fit to train\n",
    "                model.fit(X_model_features.iloc[train_idx, :], np.ravel(y.iloc[train_idx, :]))\n",
    "                \n",
    "                meta = self.model_predictor(model, X_model_features.iloc[meta_idx, :], method)\n",
    "                \n",
    "                # append metas\n",
    "                metafeatures.iloc[meta_idx] = meta\n",
    "                \n",
    "                # store fit model for future metafeature predictions\n",
    "                self.fit_blended_models[model_name].append(model)\n",
    "        \n",
    "        # append metafeatures to the metafeature dataframe\n",
    "        self.metafeatures_df[model_name] = metafeatures\n",
    "    \n",
    "    # create final df with metafeatures\n",
    "    self.X_with_metafeatures = pd.concat([X.reset_index(drop=True), self.metafeatures_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # final stacked X-fit\n",
    "    self.stacker = stacker.fit(self.X_with_metafeatures, np.ravel(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions from Fitted Ensemble\n",
    "\n",
    "This method uses the trained ensemble to make predictions on new data. It recreates the metafeatures using the fitted models, then applies the stacker for final predictions. The process differs based on whether blending was used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    prediction function using the models built in the fit method.\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert X into a dataframe if not already\n",
    "    if str(type(X)) != \"\"\"<class 'pandas.core.frame.DataFrame'>\"\"\":\n",
    "        X = pd.DataFrame(X).reset_index()\n",
    "    \n",
    "    metafeatures_df = pd.DataFrame(index=[i for i in range(X.shape[0])], columns=self.model_names_list)\n",
    "    \n",
    "    if self.blend is False:\n",
    "        \n",
    "        for (model_name, model), features, method in zip(self.fit_models.items(), self.model_feature_indices, self.model_methods):\n",
    "            \n",
    "            X_model_features = X.iloc[:, features]\n",
    "            \n",
    "            metafeatures = self.model_predictor(model, X_model_features, method)\n",
    "            \n",
    "            metafeatures_df[model_name] = metafeatures\n",
    "    \n",
    "    if self.blend is True:\n",
    "        \n",
    "        # loop through all the available model types\n",
    "        for (model_name, model_list), features, method in zip(self.fit_blended_models.items(), self.model_feature_indices, self.model_methods):\n",
    "            \n",
    "            X_model_features = X.iloc[:, features]\n",
    "            \n",
    "            model_df = pd.DataFrame()\n",
    "            \n",
    "            # loop through all the different models that were split during blended\n",
    "            for model_idx, model in enumerate(model_list):\n",
    "                \n",
    "                # predict meta\n",
    "                meta = self.model_predictor(model, X_model_features, method)\n",
    "                \n",
    "                model_df[model_idx] = meta\n",
    "            \n",
    "            # average predictions from all different models from the blending process\n",
    "            metafeatures = np.mean(model_df, axis=1)\n",
    "            \n",
    "            metafeatures_df[model_name] = metafeatures\n",
    "    \n",
    "    X_with_metafeatures = pd.concat([X.reset_index(drop=True), metafeatures_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # make predictions\n",
    "    predictions = self.stacker.predict(X_with_metafeatures)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Ensemble Learning Implementation\n",
    "\n",
    "This comprehensive example demonstrates how to implement and evaluate a complete ensemble learning pipeline using the StackerModel class. We'll compare individual models against a stacked ensemble with blending to show the performance improvements.\n",
    "\n",
    "### Import Required Libraries and Setup\n",
    "\n",
    "First, we import all necessary libraries for machine learning, including our ensemble models, evaluation metrics, and visualization tools. We also set up reproducible random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stackerpy import StackerModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(np.sum([ord(i) for i in 'StackerPy Testing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare the Dataset\n",
    "\n",
    "We load the dataset and split it into features (X) and target variable (y), then create training and testing sets with a 80/20 split for proper model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/admin/Desktop/Moje aktualne prace/Vlagun_Years/Vlagun_Total_Years4.csv\")\n",
    "\n",
    "X = df.iloc[:, 0:57].values  # feature metric\n",
    "y = df.iloc[:, 57].values\n",
    "\n",
    "X = df.iloc[:,0:57]\n",
    "y = df.iloc[:,57]\n",
    "seed = 7\n",
    "test_size = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=np.sum([ord(i) for i in 'StackerPy'])\n",
    ")\n",
    "\n",
    "# Results df creation for storage\n",
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Individual Base Models\n",
    "\n",
    "We train each base model individually to establish baseline performance metrics. This allows us to compare how much improvement the ensemble provides over individual models.\n",
    "\n",
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, np.ravel(y_train))\n",
    "random_forest_predictions = random_forest.predict(X_test)\n",
    "\n",
    "print('Original Random Forest Classifier Results')\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=y_test,\n",
    "        y_pred=random_forest_predictions\n",
    "    )\n",
    ")\n",
    "\n",
    "random_forest_accuracy = accuracy_score(y_true=y_test, y_pred=random_forest_predictions)\n",
    "random_forest_recall = recall_score(y_true=y_test, y_pred=random_forest_predictions)\n",
    "random_forest_precision = precision_score(y_true=y_test, y_pred=random_forest_predictions)\n",
    "random_forest_f1 = f1_score(y_true=y_test, y_pred=random_forest_predictions)\n",
    "\n",
    "print('Accuracy Score: ', random_forest_accuracy)\n",
    "print('Recall Score: ', random_forest_recall)\n",
    "print('Precision Score: ', random_forest_precision)\n",
    "print('F1 Score: ', random_forest_f1)\n",
    "\n",
    "print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='libfgs', max_iter=10000)\n",
    "logistic.fit(X_train, np.ravel(y_train))\n",
    "logistic_predictions = logistic.predict(X_test)\n",
    "\n",
    "print('Original Logistic Regression Classifier Results')\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=y_test,\n",
    "        y_pred=logistic_predictions\n",
    "    )\n",
    ")\n",
    "\n",
    "logistic_accuracy = accuracy_score(y_true=y_test, y_pred=logistic_predictions)\n",
    "logistic_recall = recall_score(y_true=y_test, y_pred=logistic_predictions)\n",
    "logistic_precision = precision_score(y_true=y_test, y_pred=logistic_predictions)\n",
    "logistic_f1 = f1_score(y_true=y_test, y_pred=logistic_predictions)\n",
    "\n",
    "print('Accuracy Score: ', logistic_accuracy)\n",
    "print('Recall Score: ', logistic_recall)\n",
    "print('Precision Score: ', logistic_precision)\n",
    "print('F1 Score: ', logistic_f1)\n",
    "\n",
    "print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, np.ravel(y_train))\n",
    "dtree_predictions = dtree.predict(X_test)\n",
    "\n",
    "print('Original Decision Tree Classifier Results')\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=y_test,\n",
    "        y_pred=dtree_predictions\n",
    "    )\n",
    ")\n",
    "\n",
    "dtree_accuracy = accuracy_score(y_true=y_test, y_pred=dtree_predictions)\n",
    "dtree_recall = recall_score(y_true=y_test, y_pred=dtree_predictions)\n",
    "dtree_precision = precision_score(y_true=y_test, y_pred=dtree_predictions)\n",
    "dtree_f1 = f1_score(y_true=y_test, y_pred=dtree_predictions)\n",
    "\n",
    "print('Accuracy Score: ', dtree_accuracy)\n",
    "print('Recall Score: ', dtree_recall)\n",
    "print('Precision Score: ', dtree_precision)\n",
    "print('F1 Score: ', dtree_f1)\n",
    "\n",
    "print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeClassifier()\n",
    "ridge.fit(X_train, np.ravel(y_train))\n",
    "ridge_predictions = ridge.predict(X_test)\n",
    "\n",
    "print('Original Ridge Classifier Results')\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=y_test,\n",
    "        y_pred=ridge_predictions\n",
    "    )\n",
    ")\n",
    "\n",
    "ridge_accuracy = accuracy_score(y_true=y_test, y_pred=ridge_predictions)\n",
    "ridge_recall = recall_score(y_true=y_test, y_pred=ridge_predictions)\n",
    "ridge_precision = precision_score(y_true=y_test, y_pred=ridge_predictions)\n",
    "ridge_f1 = f1_score(y_true=y_test, y_pred=ridge_predictions)\n",
    "\n",
    "print('Accuracy Score: ', ridge_accuracy)\n",
    "print('Recall Score: ', ridge_recall)\n",
    "print('Precision Score: ', ridge_precision)\n",
    "print('F1 Score: ', ridge_f1)\n",
    "\n",
    "print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train the Stacked Ensemble with Blending\n",
    "\n",
    "Now we create our ensemble using the StackerModel with blending enabled. The base models generate metafeatures through cross-validation, and a Ridge classifier serves as the final stacker to combine their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacker Model classifier results / eval with blending\n",
    "lr2 = LogisticRegression(solver='libfgs', max_iter=10000)\n",
    "dt2 = DecisionTreeClassifier()\n",
    "rf2 = RandomForestClassifier(n_estimators=25)\n",
    "rc2 = RidgeClassifier()\n",
    "models = [lr2, dt2, rf2]\n",
    "\n",
    "stacker = StackerModel()\n",
    "stacker.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    models=models,\n",
    "    stacker=rc2,\n",
    "    blend=True,\n",
    "    splits=5,\n",
    "    model_feature_indices=None\n",
    ")\n",
    "\n",
    "stacker_predictions = stacker.predict(X_test)\n",
    "\n",
    "print('Stacker Model Results with blending')\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=y_test,\n",
    "        y_pred=stacker_predictions\n",
    "    )\n",
    ")\n",
    "\n",
    "stacker_accuracy = accuracy_score(y_true=y_test, y_pred=stacker_predictions)\n",
    "stacker_recall = recall_score(y_true=y_test, y_pred=stacker_predictions)\n",
    "stacker_precision = precision_score(y_true=y_test, y_pred=stacker_predictions)\n",
    "stacker_f1 = f1_score(y_true=y_test, y_pred=stacker_predictions)\n",
    "\n",
    "print('Accuracy Score: ', stacker_accuracy)\n",
    "print('Recall Score: ', stacker_recall)\n",
    "print('Precision Score: ', stacker_precision)\n",
    "print('F1 Score: ', stacker_f1)\n",
    "\n",
    "print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Visualize Results\n",
    "\n",
    "Finally, we organize all model performance metrics into a structured format and create visualizations to compare the effectiveness of individual models versus the ensemble approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results for comparison\n",
    "results['model'] = ['RandomForest', 'LogisticRegression', 'DecisionTree', 'RidgeClassifier', 'StackerModel']\n",
    "\n",
    "results['accuracy'] = [random_forest_accuracy, logistic_accuracy, dtree_accuracy, ridge_accuracy, stacker_accuracy]\n",
    "\n",
    "results['recall'] = [random_forest_recall, logistic_recall, dtree_recall, ridge_recall, stacker_recall]\n",
    "\n",
    "results['precision'] = [random_forest_precision, logistic_precision, dtree_precision, ridge_precision, stacker_precision]\n",
    "\n",
    "results['f1'] = [random_forest_f1, logistic_f1, dtree_f1, ridge_f1, stacker_f1]\n",
    "\n",
    "# Create performance comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "((ax1, ax2), (ax3, ax4)) = axes\n",
    "\n",
    "for ax, score in zip([ax1, ax2, ax3, ax4], results.columns[1:]):\n",
    "    ax.set_title(f'{score} Score')\n",
    "    ax.set_ylabel('Score')\n",
    "    results.set_index('model')[score].sort_values().plot(kind='bar', ax=ax, fontsize=10, color='#2E73A0')\n",
    "    ax.set_xlabel('Model Name')\n",
    "    ax.set_ylim([0.7, 1])\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}